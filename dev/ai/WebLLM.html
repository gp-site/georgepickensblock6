<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>WebLLM (ESM Import)</title>
  <style>
    body { font-family: system-ui; margin: 2rem; max-width: 780px; }
    textarea { width:100%; min-height:120px; }
    pre { background:#111; color:#eee; padding:1rem; white-space:pre-wrap; }
    button { padding:0.6rem 1rem; }
  </style>
</head>
<body>
  <h1>WebLLM Demo (ESM)</h1>
  <p>Runs a quantized model locally. First load may take time (downloads weights).</p>
  <label>Prompt:</label><br>
  <textarea id="prompt">Explain what a transformer model is in simple terms.</textarea><br>
  <button id="runBtn">Run</button>
  <pre id="out">Loading module...</pre>

  <script type="module">
    // Import everything under an alias so we know exactly what we have.
    import * as webllm from "https://unpkg.com/@mlc-ai/web-llm@0.2.61/dist/index.js";

    const out = document.getElementById('out');
    const runBtn = document.getElementById('runBtn');

    // Pick a model that exists in the release. (Adjust for newer versions if needed.)
    const MODEL = 'Mistral-7B-Instruct-q4f16_1';

    let engine;
    async function init() {
      try {
        out.textContent = 'Initializing model (this triggers weight downloads)...';
        engine = await webllm.CreateMLCEngine(MODEL, {
          initProgressCallback: (p) => {
            out.textContent = `Loading: ${(p.progress * 100).toFixed(1)}% - ${p.text}`;
          }
        });
        out.textContent = 'Model ready. Enter a prompt and click Run.';
      } catch (e) {
        out.textContent = 'Init error: ' + e.message + '\n(Check DevTools Network tab.)';
      }
    }

    runBtn.addEventListener('click', async () => {
      if (!engine) return;
      const prompt = document.getElementById('prompt').value.trim();
      if (!prompt) return;
      out.textContent = 'Generating...\n';
      let result = '';
      try {
        const stream = await engine.chat.completions.create({
          messages: [{ role: 'user', content: prompt }],
          stream: true,
          temperature: 0.7,
          max_tokens: 300
        });
        for await (const part of stream) {
          const delta = part?.choices?.[0]?.delta?.content;
            if (delta) {
              result += delta;
              out.textContent = result;
            }
        }
      } catch (e) {
        out.textContent = 'Generation error: ' + e.message;
      }
    });

    init();
  </script>
</body>
</html>